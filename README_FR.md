# AI Snake avec PPO (Proximal Policy Optimization)

Une IA qui apprend Ã  jouer au jeu Snake en utilisant l'algorithme PPO (Proximal Policy Optimization).

## ğŸ® PrÃ©sentation

Ce projet implÃ©mente un agent d'apprentissage par renforcement qui apprend Ã  jouer au jeu Snake. L'algorithme utilisÃ© est **PPO** (Proximal Policy Optimization), une mÃ©thode state-of-the-art pour l'apprentissage par renforcement.

### CaractÃ©ristiques

- âœ… **Algorithme PPO complet** avec Actor-Critic
- âœ… **Support GPU** pour accÃ©lÃ©rer l'entraÃ®nement (CUDA)
- âœ… **GAE** (Generalized Advantage Estimation) pour de meilleures estimations
- âœ… **Sauvegarde/Chargement** automatique des modÃ¨les
- âœ… **Visualisation** de l'entraÃ®nement avec graphiques Excel
- âœ… **Mode visible/invisible** pour accÃ©lÃ©rer l'entraÃ®nement

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- CUDA (optionnel, pour utiliser le GPU)
- Carte graphique NVIDIA (optionnel, pour l'accÃ©lÃ©ration)

## ğŸš€ Installation

1. **Cloner le repository** (ou tÃ©lÃ©charger les fichiers)

2. **Installer les dÃ©pendances** :
```bash
pip install -r requirements.txt
```

3. **VÃ©rifier l'installation de PyTorch avec CUDA** (optionnel) :
```bash
python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')"
```

## ğŸ¯ Utilisation

### EntraÃ®ner l'IA

Pour lancer l'entraÃ®nement :

```bash
python main.py
```

L'entraÃ®nement va :
- CrÃ©er un agent PPO
- EntraÃ®ner pendant 10 000 Ã©pisodes (configurable dans `ia.py`)
- Sauvegarder le modÃ¨le automatiquement tous les 100 Ã©pisodes
- CrÃ©er un fichier Excel avec les statistiques d'entraÃ®nement

### Configuration

Vous pouvez modifier les paramÃ¨tres dans `snake.py` et `ia.py` :

#### Dans `snake.py` :
```python
show = False  # True pour voir le jeu, False pour entraÃ®ner plus vite
player = False  # True pour jouer manuellement
stop_iteration = 100  # Nombre max de mouvements par partie
```

#### Dans `ia.py` :
```python
nb_loop_train = 10000  # Nombre d'Ã©pisodes d'entraÃ®nement
gamma = 0.99  # Facteur de discount
epsilon_clip = 0.2  # Clipping PPO
learning_rate = 3e-4  # Taux d'apprentissage
```

### Regarder l'IA jouer

Une fois l'entraÃ®nement terminÃ©, vous pouvez regarder l'IA jouer :

1. Dans `snake.py`, changez :
```python
show = True  # Activer l'affichage
```

2. CrÃ©ez un script de test (par exemple `test.py`) :
```python
import snake
import ia

# Charger le modÃ¨le entraÃ®nÃ©
agent = ia.create_agent(16, 4)
agent.load_model("models_ppo1/snake_ppo_model.pth")

# Jouer une partie
score = snake.game_loop(snake.rect_width, snake.rect_height, snake.display, agent)
print(f"Score final: {score}")
```

## ğŸ§  Architecture de l'IA

### Ã‰tat (Observations)
L'agent observe 16 valeurs :
- 8 distances aux obstacles (murs ou corps du serpent) dans 8 directions
- 8 distances Ã  la nourriture dans 8 directions

### Actions
4 actions possibles :
- 0 : Haut (UP)
- 1 : Droite (RIGHT)
- 2 : Bas (DOWN)
- 3 : Gauche (LEFT)

### RÃ©compenses
- **+1** : Manger la nourriture
- **-1** : Mourir (collision avec mur ou soi-mÃªme)

### RÃ©seau de neurones

**Architecture Actor-Critic** :
```
Input (16)
    â†“
Shared layers: Linear(16â†’256) â†’ ReLU â†’ Linear(256â†’256) â†’ ReLU
    â†“
    â”œâ†’ Actor: Linear(256â†’128) â†’ ReLU â†’ Linear(128â†’4) â†’ Softmax
    â””â†’ Critic: Linear(256â†’128) â†’ ReLU â†’ Linear(128â†’1)
```

## ğŸ“Š RÃ©sultats

Les rÃ©sultats de l'entraÃ®nement sont sauvegardÃ©s dans :
- **ModÃ¨les** : `models_ppoX/snake_ppo_model.pth`
- **Graphiques** : `donnees2.xlsx` avec un graphique d'Ã©volution du score

## ğŸ”§ Fichiers du projet

- `ia.py` : ImplÃ©mentation de l'agent PPO
- `snake.py` : Jeu Snake et environnement
- `main.py` : Script d'entraÃ®nement principal
- `exw.py` : Utilitaires pour Excel
- `compteur.py` : Compteur d'exÃ©cutions
- `requirements.txt` : DÃ©pendances Python

## ğŸ’¡ Conseils d'optimisation

1. **Pour un entraÃ®nement plus rapide** :
   - Mettez `show = False` dans `snake.py`
   - Augmentez `stop_iteration` pour des parties plus longues
   - Utilisez un GPU (CUDA)

2. **Pour amÃ©liorer les performances** :
   - Ajustez `learning_rate` (essayez 1e-4 ou 5e-4)
   - Modifiez `gamma` (essayez 0.95 ou 0.99)
   - Augmentez `nb_loop_train` pour plus d'entraÃ®nement

3. **Si l'IA n'apprend pas** :
   - VÃ©rifiez que le GPU est bien utilisÃ©
   - RÃ©duisez `epsilon_clip` (essayez 0.1)
   - Augmentez `c2` (entropy bonus) pour plus d'exploration

## ğŸ“ Algorithme PPO

PPO (Proximal Policy Optimization) est un algorithme d'apprentissage par renforcement qui :
1. Collecte des trajectoires en jouant avec la politique actuelle
2. Calcule les avantages avec GAE
3. Met Ã  jour la politique avec un objectif clippÃ© pour Ã©viter les mises Ã  jour trop importantes
4. Met Ã  jour le critique pour mieux estimer les valeurs

## ğŸ¤ Contribution

N'hÃ©sitez pas Ã  modifier et amÃ©liorer le code !

## ğŸ“„ Licence

Ce projet est libre d'utilisation.
